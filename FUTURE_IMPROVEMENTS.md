# Future Improvements & TODO

## Project Goal
Build a production-ready reward model for evaluating egocentric video clips for robot manipulation training, using a diverse dataset from multiple sources.

---

## ğŸ¥ **Data Sources & Infrastructure**

### Current State
- Using single EPIC-Kitchens-100 video (P01_102.mp4) locally
- Manual download process
- Limited to 20 clips (10 annotated + 10 random)

### Required Changes

#### 1. **Cloud Video Storage**
- [ ] Set up cloud storage (AWS S3, Google Cloud Storage, or Azure Blob)
- [ ] Upload diverse video dataset:
  - EPIC-Kitchens-100 videos
  - Ego4D dataset videos
  - Self-recorded GoPro footage
- [ ] Organize by source and metadata
- [ ] Implement access controls and versioning

**Structure:**
```
cloud-storage/
â”œâ”€â”€ epic_kitchens/
â”‚   â”œâ”€â”€ P01_101.mp4
â”‚   â”œâ”€â”€ P01_102.mp4
â”‚   â””â”€â”€ ...
â”œâ”€â”€ ego4d/
â”‚   â”œâ”€â”€ video_001.mp4
â”‚   â””â”€â”€ ...
â””â”€â”€ gopro/
    â”œâ”€â”€ session_001.mp4
    â””â”€â”€ ...
```

#### 2. **Video Download Script (00_download_videos.py)**
- [ ] Replace `01_download_sample.py` with robust downloader
- [ ] Download from cloud storage instead of manual process
- [ ] Support multiple data sources (EPIC, Ego4D, GoPro)
- [ ] Implement:
  - Progress tracking
  - Resume capability
  - Parallel downloads
  - Checksum verification
  - Automatic retry on failure
- [ ] Store videos in organized local structure
- [ ] **Do NOT commit MP4 files to git** (use .gitignore)

**New Script Features:**
```python
# 00_download_videos.py
- download_from_cloud(source='epic_kitchens', video_ids=[...])
- download_from_cloud(source='ego4d', video_ids=[...])
- download_from_cloud(source='gopro', session_ids=[...])
- verify_downloads()
- cleanup_old_videos()
```

#### 3. **Metadata Management (01_prepare_metadata.py)**
- [ ] Create unified metadata file for all sources
- [ ] Map annotations across different datasets:
  - EPIC-Kitchens: CSV annotations
  - Ego4D: JSON annotations
  - GoPro: Manual annotations or auto-generated
- [ ] Standardize action labels across datasets
- [ ] Track video source, quality metrics, action types

**Unified Metadata Format:**
```json
{
  "clip_id": "epic_P01_102_001",
  "source": "epic_kitchens",
  "video_id": "P01_102",
  "start_frame": 1545,
  "stop_frame": 1866,
  "action": "wash knife",
  "verb": "wash",
  "noun": "knife",
  "original_annotation": {...}
}
```

---

## ğŸ“ **Repository Structure Redesign**

### Current Structure
```
data_eval_engine/
â”œâ”€â”€ epic_prototype/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ videos/          # Local videos (gitignored)
â”‚   â”‚   â””â”€â”€ mixed_clips/
â”‚   â””â”€â”€ scripts/
â””â”€â”€ human_annotation_tool/
```

### Proposed New Structure
```
data_eval_engine/
â”œâ”€â”€ data/                              # All data (gitignored)
â”‚   â”œâ”€â”€ raw_videos/                    # Downloaded source videos
â”‚   â”‚   â”œâ”€â”€ epic_kitchens/
â”‚   â”‚   â”œâ”€â”€ ego4d/
â”‚   â”‚   â””â”€â”€ gopro/
â”‚   â”œâ”€â”€ annotations/                   # Source annotations
â”‚   â”‚   â”œâ”€â”€ epic_100_train.csv
â”‚   â”‚   â”œâ”€â”€ ego4d_annotations.json
â”‚   â”‚   â””â”€â”€ gopro_manual_labels.csv
â”‚   â”œâ”€â”€ extracted_clips/               # Processed clips
â”‚   â”‚   â”œâ”€â”€ annotated/
â”‚   â”‚   â””â”€â”€ random/
â”‚   â””â”€â”€ metadata/
â”‚       â”œâ”€â”€ unified_metadata.json
â”‚       â”œâ”€â”€ clip_features.csv
â”‚       â””â”€â”€ download_manifest.json
â”‚
â”œâ”€â”€ scripts/                           # All processing scripts
â”‚   â”œâ”€â”€ 00_download_videos.py         # NEW: Download from cloud
â”‚   â”œâ”€â”€ 01_prepare_metadata.py        # NEW: Unify annotations
â”‚   â”œâ”€â”€ 02_extract_clips.py           # UPDATED: Multi-source extraction
â”‚   â”œâ”€â”€ 03_compute_features.py
â”‚   â”œâ”€â”€ 04_visualize_clip.py
â”‚   â”œâ”€â”€ 05_extract_mixed_clips.py
â”‚   â”œâ”€â”€ 06_compute_all_features.py
â”‚   â””â”€â”€ 07_reencode_clips_for_web.py
â”‚
â”œâ”€â”€ annotation_tool/                   # Renamed from human_annotation_tool
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ templates/
â”‚   â””â”€â”€ results/
â”‚
â”œâ”€â”€ reward_model/                      # NEW: Model training
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ configs/                           # NEW: Configuration files
â”‚   â”œâ”€â”€ cloud_storage.yaml
â”‚   â”œâ”€â”€ data_sources.yaml
â”‚   â””â”€â”€ model_config.yaml
â”‚
â”œâ”€â”€ notebooks/                         # Analysis notebooks
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”‚
â”œâ”€â”€ tests/                             # Unit tests
â”‚   â””â”€â”€ test_data_processing.py
â”‚
â”œâ”€â”€ .gitignore                         # Ignore videos, models
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ FUTURE_IMPROVEMENTS.md             # This file
```

---

## ğŸ”§ **Script Updates Required**

### 02_extract_clips.py (formerly 02_extract_single_clip.py)
**Current:** Extracts single clip from one video
**Needed:**
- [ ] Accept multiple video sources
- [ ] Handle different annotation formats
- [ ] Batch extraction across all videos
- [ ] Support different FPS and resolutions
- [ ] Maintain source tracking in metadata

```python
# Updated interface
extract_clips(
    source='epic_kitchens',
    video_ids=['P01_101', 'P01_102'],
    clip_type='annotated',  # or 'random'
    output_dir='data/extracted_clips/'
)
```

### 05_extract_mixed_clips.py
**Current:** 50/50 mix from single video
**Needed:**
- [ ] Sample clips across ALL video sources
- [ ] Stratified sampling by source and action type
- [ ] Configurable ratios (not just 50/50)
- [ ] Ensure diversity in action types

---

## ğŸ§  **Reward Model Training** (NEW)

### To Implement
- [ ] Create `reward_model/train.py`
- [ ] Use pairwise preferences from annotations
- [ ] Train MLP on extracted features
- [ ] Implement Bradley-Terry model or similar
- [ ] Cross-validation and evaluation metrics
- [ ] Model checkpointing
- [ ] Hyperparameter tuning

### Model Architecture
```python
# Input: Concatenated features from both clips
# - Visual quality (2 features)
# - Motion (2 features)
# - Hand detection (1 feature)
# - CLIP embedding (512 features)
# Total: ~517 features per clip Ã— 2 = 1034 input features

# Output: Preference probability (which clip is better)
```

---

## ğŸ“Š **Annotation Tool Improvements**

### Current State
- Basic pairwise comparison
- 30 annotations collected
- Single annotator

### Future Improvements
- [ ] Multi-annotator support with tracking
- [ ] Inter-annotator agreement metrics (Cohen's Kappa)
- [ ] Active learning: prioritize uncertain pairs
- [ ] Show clip features during annotation (optional)
- [ ] Export to common formats (Labelbox, CVAT)
- [ ] Progress dashboard with coverage heatmap
- [ ] Keyboard shortcuts documentation page

---

## ğŸ”¬ **Feature Extraction Improvements**

### Current Features
- Blur score (Laplacian variance)
- Exposure score (histogram entropy)
- Motion variance (optical flow)
- Jerk score (motion smoothness)
- Hand visibility (MediaPipe)
- CLIP embeddings (512-D)

### Additional Features to Consider
- [ ] Object detection (detect tools, ingredients)
- [ ] Action recognition (pre-trained model scores)
- [ ] Audio features (if available)
- [ ] 3D hand pose estimation
- [ ] Scene change detection
- [ ] Temporal consistency metrics
- [ ] Task-specific features (grasp quality, object manipulation)

---

## ğŸ—ï¸ **Infrastructure & Deployment**

### Development Environment
- [ ] Docker containerization
- [ ] Requirements pinning and dependency management
- [ ] Environment variables for cloud credentials
- [ ] Logging and monitoring setup

### Cloud Deployment
- [ ] Deploy annotation tool to cloud (Heroku, AWS, etc.)
- [ ] Set up CI/CD pipeline
- [ ] Automated testing
- [ ] Model serving API (FastAPI)

---

## ğŸ“ **Documentation Needs**

- [ ] API documentation for scripts
- [ ] Data schema documentation
- [ ] Model training guide
- [ ] Annotation guidelines for new annotators
- [ ] Deployment guide
- [ ] Contributing guidelines

---

## âš ï¸ **Known Issues & Considerations**

### Current Issues
1. **Composite score doesn't reflect task utility**
   - High sharpness doesn't mean useful for robot training
   - Need human labels to learn what matters

2. **Video codec compatibility**
   - Had to re-encode from mp4v to H.264
   - Future: extract directly to H.264

3. **Limited diversity**
   - Single video source
   - Limited action types
   - Need more varied scenarios

### Design Decisions to Revisit
- [ ] Should we weight features in composite score?
- [ ] How to handle ties in pairwise comparisons?
- [ ] Optimal clip duration (currently 3s)?
- [ ] Resolution vs. file size tradeoff (currently 480x480)?
- [ ] FPS for analysis (currently 15fps)?

---

## ğŸ“… **Implementation Priority**

### Phase 1: Data Infrastructure (Next Sprint)
1. Set up cloud storage
2. Create download scripts
3. Restructure repository
4. Update .gitignore

### Phase 2: Multi-Source Support
1. Unify metadata across sources
2. Update extraction scripts
3. Collect diverse clips

### Phase 3: Scale Annotation
1. Deploy annotation tool
2. Recruit multiple annotators
3. Collect 200-500 annotations

### Phase 4: Model Training
1. Implement reward model
2. Train on collected preferences
3. Evaluate and iterate

### Phase 5: Production
1. Model serving API
2. Integration with robot training pipeline
3. Continuous improvement loop

---

## ğŸ”— **External Resources**

- EPIC-Kitchens-100: https://epic-kitchens.github.io/
- Ego4D: https://ego4d-data.org/
- Reward Learning Papers: [Add relevant papers]
- Cloud Storage Docs: [Add links when chosen]

---

## ğŸ’¡ **Ideas to Explore**

- Active learning for annotation efficiency
- Self-supervised pre-training on unlabeled videos
- Multi-task learning (predict both quality and action)
- Temporal modeling (not just single clips)
- Cross-dataset transfer learning
- Uncertainty estimation in reward predictions

---

**Last Updated:** 2025-11-08
**Status:** Work in Progress - Prototype Phase Complete
